# src/ava/services/generation_coordinator.py
"""
Coordinates the multi-step code generation process, from planning to final review.
Emits an event for each file before it's generated to enable real-time UI updates.
"""
import asyncio
import json
import logging
import re
from typing import Dict, Any, Optional, List, Set

from src.ava.core.event_bus import EventBus
from src.ava.prompts import REVIEWER_PROMPT
from src.ava.prompts.planner import CODE_SNIPPET_GENERATOR_PROMPT

logger = logging.getLogger(__name__)


class GenerationCoordinator:
    """
    Manages the dynamic, multi-step 'Whiteboard' workflow for code generation
    and orchestrates the final 'Review and Correct' pass. Emits events for
    real-time UI updates during the generation process.
    """

    def __init__(self, service_manager: Any, event_bus: EventBus, context_manager: Any,
                 import_fixer_service: Any, integration_validator: Any):
        """
        Initializes the GenerationCoordinator.

        Args:
            service_manager: The main service manager for accessing other services.
            event_bus: The application's event bus for communication.
            context_manager: Manages the context for generation tasks.
            import_fixer_service: Service to fix Python imports.
            integration_validator: Service to validate code integration.
        """
        self.service_manager = service_manager
        self.event_bus = event_bus
        self.context_manager = context_manager
        self.import_fixer_service = import_fixer_service
        self.integration_validator = integration_validator
        self.llm_client = service_manager.get_llm_client()

    def log(self, level: str, message: str):
        """
        Logs a message and emits a status update event.

        Args:
            level: The logging level (e.g., 'info', 'warning', 'error').
            message: The message to log.
        """
        log_level = getattr(logging, level.upper(), logging.INFO)
        logger.log(log_level, message)
        self.event_bus.emit("status_update", {"level": level, "message": message})

    async def coordinate_generation(self, whiteboard_plan: Dict[str, Any],
                                    existing_files: Optional[Dict[str, str]]) -> Dict[str, str]:
        """
        Coordinates the entire code generation process based on a whiteboard plan.

        This method orchestrates the execution of tasks, fixes imports, and performs
        a final review and correction pass. It emits events to update the UI in
        real-time.

        Args:
            whiteboard_plan: The plan generated by the Architect, containing tasks.
            existing_files: A dictionary of existing file contents.

        Returns:
            A dictionary of the final, generated file contents.
        """
        try:
            self.log("info", "ðŸš€ Starting Whiteboard generation workflow...")
            generated_files_this_session = (existing_files or {}).copy()
            tasks = whiteboard_plan.get("tasks", [])
            total_tasks = len(tasks)
            emitted_files: Set[str] = set()  # Track files for which generation has started

            for i, task in enumerate(tasks):
                filename = task.get("filename")
                # Emit event for a file only once, before its first generation task
                if filename and filename not in emitted_files:
                    self.event_bus.emit("file_generation_starting", {"filename": filename})
                    emitted_files.add(filename)
                    await asyncio.sleep(0.1)  # Brief pause for UI responsiveness

                self.log("info", f"Executing task {i + 1}/{total_tasks}: {task.get('description')}")
                generated_files_this_session = await self._execute_task(task, generated_files_this_session)
                self.event_bus.emit("coordinated_generation_progress",
                                    {"filename": task.get('filename', 'Plan'), "completed": i + 1,
                                     "total": total_tasks})

            self.log("info", "Running post-generation import fixing pass...")
            self.event_bus.emit("agent_status_changed", {"agent": "Fixer", "status": "Fixing imports...", "icon": "fa5s.wrench"})
            fixed_files = {}
            project_indexer = self.service_manager.get_project_indexer_service()
            project_manager = self.service_manager.get_project_manager()
            project_index = project_indexer.build_index(
                project_manager.active_project_path) if project_manager.active_project_path else {}

            for filename, content in generated_files_this_session.items():
                if filename.endswith('.py'):
                    module_path = filename.replace('.py', '').replace('/', '.')
                    fixed_content = self.import_fixer_service.fix_imports(content, project_index, module_path)
                    fixed_files[filename] = fixed_content
                else:
                    fixed_files[filename] = content
            generated_files_this_session = fixed_files
            self.log("success", "âœ… Import fixing pass complete.")

            self.log("info", "ðŸ”¬ Starting final 'Review and Correct' pass...")
            final_files = await self._perform_review_and_correct(generated_files_this_session)

            self.log("success", f"âœ… Whiteboard generation complete. {len(final_files)} files processed.")
            return final_files

        except Exception as e:
            self.log("error", f"Coordinated generation failed: {e}")
            logger.exception("Exception in coordinate_generation")
            return {}

    async def _execute_task(self, task: Dict[str, Any], current_files: Dict[str, str]) -> Dict[str, str]:
        """
        Executes a single task from the whiteboard plan (e.g., create, insert, replace).

        Args:
            task: The task dictionary from the plan.
            current_files: The current state of all generated files.

        Returns:
            The updated state of all generated files after executing the task.
        """
        filename = task.get("filename")
        if not filename:
            self.log("warning", f"Task '{task.get('description')}' has no filename. Skipping.")
            return current_files

        # Event emission is now handled in coordinate_generation to fire only once per file.

        original_content = current_files.get(filename, "")

        self.event_bus.emit("agent_status_changed", {"agent": "Coder", "status": f"Implementing: {task.get('description')}", "icon": "fa5s.code"})
        snippet = await self._generate_code_snippet(task, current_files)
        if snippet is None:
            snippet = f"\n# ERROR: Failed to generate code for task: {task.get('description')}\n"

        if task.get("type") != "create_file" and snippet and not snippet.endswith('\n'):
            snippet += '\n'

        if task.get("type") == "create_file":
            new_content = snippet
            await self._stream_content(filename, new_content, clear_first=True)
        else:
            lines = original_content.splitlines(True)
            if original_content.endswith('\n') and (not lines or not lines[-1].endswith('\n')):
                lines.append('\n')

            start_line = task.get("start_line", 1)
            end_line = task.get("end_line", start_line)
            start_idx = max(0, start_line - 1)
            end_idx = min(len(lines), end_line)

            self.event_bus.emit("highlight_lines_for_edit", {"filename": filename, "start_line": start_line, "end_line": end_line})
            await asyncio.sleep(0.5)
            self.event_bus.emit("delete_highlighted_lines", {"filename": filename})
            await asyncio.sleep(0.3)
            self.event_bus.emit("position_cursor_for_insert", {"filename": filename, "line": start_line, "column": 0})
            await asyncio.sleep(0.1)

            await self._stream_content(filename, snippet)

            new_lines = lines[:start_idx] + snippet.splitlines(True) + lines[end_idx:]
            new_content = "".join(new_lines)

        current_files[filename] = new_content
        return current_files

    async def _stream_content(self, filename: str, content: str, clear_first: bool = False):
        """
        Streams content to the UI editor for a given file.

        Args:
            filename: The name of the file being edited.
            content: The content to stream.
            clear_first: Whether to clear the editor before streaming.
        """
        if clear_first:
            self.event_bus.emit("clear_editor_content", {"filename": filename})
            await asyncio.sleep(0.05)

        self.event_bus.emit("stream_editor_content", {"filename": filename, "content": content})
        await asyncio.sleep(0.1)

    async def _generate_code_snippet(self, task: Dict[str, Any], current_files: Dict[str, str]) -> Optional[str]:
        """
        Generates a code snippet for a specific task using the LLM.

        Args:
            task: The task dictionary from the plan.
            current_files: The current state of all generated files for context.

        Returns:
            The generated code snippet as a string, or None on failure.
        """
        prompt = self._build_snippet_generation_prompt(task, current_files)
        provider, model = self.llm_client.get_model_for_role("coder")
        if not provider or not model:
            self.log("error", f"No model for 'coder' role. Cannot generate snippet for {task.get('filename')}.")
            return None

        snippet = ""
        try:
            async for chunk in self.llm_client.stream_chat(provider, model, prompt, "coder"):
                snippet += chunk
            return self.robust_clean_llm_output(snippet)
        except Exception as e:
            self.log("error", f"LLM snippet generation failed for {task.get('filename')}: {e}")
            logger.exception("Exception in _generate_code_snippet")
            return None

    def _build_snippet_generation_prompt(self, task: Dict[str, Any], current_files: Dict[str, str]) -> str:
        """
        Builds the prompt for the LLM to generate a code snippet.

        Args:
            task: The task dictionary from the plan.
            current_files: The current state of all generated files for context.

        Returns:
            The fully formatted prompt string.
        """
        filename = task.get("filename")
        file_content = current_files.get(filename, "# This is a new file.")
        other_files_context = {k: v for k, v in current_files.items() if k != filename}
        return CODE_SNIPPET_GENERATOR_PROMPT.format(
            task_json=json.dumps(task, indent=2),
            filename=filename,
            file_content=file_content,
            code_context_json=json.dumps(other_files_context, indent=2)
        )

    async def _perform_review_and_correct(self, generated_files: Dict[str, str]) -> Dict[str, str]:
        """
        Performs a final review pass on all generated code, making corrections.

        Args:
            generated_files: A dictionary of all generated file contents.

        Returns:
            A dictionary of the corrected file contents.
        """
        self.event_bus.emit("agent_status_changed", {"agent": "Reviewer", "status": "Analyzing generated code...", "icon": "fa5s.search"})
        prompt = REVIEWER_PROMPT.format(code_to_review_json=json.dumps(generated_files, indent=2))
        provider, model = self.llm_client.get_model_for_role("reviewer")
        if not provider or not model:
            self.log("error", "No model for 'reviewer' role. Skipping review pass.")
            return generated_files

        raw_response = ""
        try:
            async for chunk in self.llm_client.stream_chat(provider, model, prompt, "reviewer"):
                raw_response += chunk

            review_result = self.robust_parse_json_response(raw_response)
            if not review_result or not review_result.get("issues"):
                self.log("success", "âœ… Review complete. No issues found.")
                return generated_files

            self.log("info", f"Review found {len(review_result['issues'])} issues. Applying corrections...")
            corrected_files = generated_files.copy()
            for issue in review_result.get("issues", []):
                filename = issue.get("file")
                if filename in corrected_files:
                    corrected_files[filename] = issue.get("corrected_code", corrected_files[filename])
                    self.log("info", f"Applied correction to {filename}: {issue.get('description')}")
            return corrected_files

        except Exception as e:
            self.log("error", f"Review and correct pass failed: {e}")
            logger.exception("Exception in _perform_review_and_correct")
            return generated_files

    @staticmethod
    def robust_clean_llm_output(raw_output: str) -> str:
        """
        Cleans the raw output from the LLM, removing markdown code fences.

        Args:
            raw_output: The raw string response from the LLM.

        Returns:
            The cleaned code or text content.
        """
        code_block_regex = re.compile(r"```(?:[a-zA-Z0-9]+)?\n(.*?)```", re.DOTALL)
        match = code_block_regex.search(raw_output)
        if match:
            return match.group(1).strip()
        return raw_output.strip().strip('`')

    def robust_parse_json_response(self, raw_response: str) -> Dict[str, Any]:
        """
        Parses a JSON response from an LLM, attempting to fix common formatting errors.

        Args:
            raw_response: The raw string which is expected to contain a JSON object.

        Returns:
            A dictionary parsed from the JSON, or an empty dictionary if parsing fails.
        """
        try:
            json_match = re.search(r"```(?:json)?\n({.*?})\n```", raw_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                start_index = raw_response.find('{')
                end_index = raw_response.rfind('}')
                if start_index != -1 and end_index != -1:
                    json_str = raw_response[start_index:end_index + 1]
                else:
                    self.log("warning", "No JSON object found in the LLM response.")
                    return {}

            json_str = re.sub(r",\s*([}\]])", r"\1", json_str)
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            self.log("error", f"Failed to parse JSON from LLM response: {e}")
            logger.error(f"Problematic JSON string for parsing: {raw_response}")
            return {}